## Introduction

This project implements a Seq2Seq (Sequence-to-Sequence) model using Recurrent Neural Networks (RNNs) for machine translation. The model, built with TensorFlow and Keras, translates sentences from one language to another. It features an encoder-decoder architecture using Long Short-Term Memory (LSTM) units, making it suitable for handling sequences of varying lengths and learning complex mappings between languages.

## Working Principle

The Seq2Seq model operates in two main phases:

1. **Encoding**: The encoder processes the input sequence (source language sentence) and compresses it into a fixed-size context vector, which captures the semantic meaning of the entire sequence. This is achieved using LSTM layers.

2. **Decoding**: The decoder takes the context vector and generates the output sequence (target language sentence) step-by-step. It uses LSTM layers to predict the next token in the sequence based on the previous tokens and the context vector.

The model uses the encoder to create a representation of the input sequence and the decoder to generate the translated output. An attention mechanism (if implemented) could further enhance the model by allowing the decoder to focus on different parts of the input sequence for each token it generates.

## Model Architecture

The Seq2Seq model is composed of two main components:

1. **Encoder**:
   - **Embedding Layer**: Converts input tokens into dense vectors of fixed size.
   - **LSTM Layer**: Processes the embedded input sequence and outputs hidden states and cell states that encapsulate the sequence information.

2. **Decoder**:
   - **Embedding Layer**: Converts target tokens into dense vectors of fixed size.
   - **LSTM Layer**: Takes the encoder's output and generates the sequence of target tokens step-by-step.
   - **TimeDistributed Layer**: Applies a Dense layer to each time step of the LSTM output to produce a probability distribution over the vocabulary for the next token prediction.

The encoder and decoder models are linked through the context vector, which captures the input sequence's meaning and is used to initialize the decoder's hidden states.

## Input and Output

### Input
- **Source Language Sentence**: A sequence of words in the source language(English), tokenized and padded to a fixed length.`

### Output
- **Target Language Sentence**: The translated sequence in the target language(Bengali), generated by the model.`

The input is processed by the encoder to produce a context vector, which is then used by the decoder to generate the translated sentence, token by token.

## Use Cases

1. **Machine Translation**: Translate sentences from one language to another. Useful for building translation services or tools.

## Data Set
https://www.kaggle.com/datasets/sayedshaun/english-to-bengali-for-machine-translation
